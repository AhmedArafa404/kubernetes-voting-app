
==> Audit <==
|---------|------------------------|----------|------------|---------|----------------------|----------------------|
| Command |          Args          | Profile  |    User    | Version |      Start Time      |       End Time       |
|---------|------------------------|----------|------------|---------|----------------------|----------------------|
| start   | --driver=docker        | minikube | ahmedarafa | v1.36.0 | 12 Jul 25 06:00 EEST |                      |
| start   | --driver=docker        | minikube | ahmedarafa | v1.36.0 | 12 Jul 25 14:17 EEST | 12 Jul 25 14:26 EEST |
| start   | --driver=docker        | minikube | ahmedarafa | v1.36.0 | 12 Jul 25 14:30 EEST |                      |
| service | vote-service -n vote   | minikube | ahmedarafa | v1.36.0 | 12 Jul 25 14:48 EEST | 12 Jul 25 14:48 EEST |
| service | result-service -n vote | minikube | ahmedarafa | v1.36.0 | 12 Jul 25 14:52 EEST |                      |
|---------|------------------------|----------|------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/07/12 14:30:26
Running on machine: ahmedarafa-VMware-Virtual-Platform
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0712 14:30:26.620910   11254 out.go:345] Setting OutFile to fd 1 ...
I0712 14:30:26.621956   11254 out.go:397] isatty.IsTerminal(1) = true
I0712 14:30:26.621969   11254 out.go:358] Setting ErrFile to fd 2...
I0712 14:30:26.621985   11254 out.go:397] isatty.IsTerminal(2) = true
I0712 14:30:26.623171   11254 root.go:338] Updating PATH: /home/ahmedarafa/.minikube/bin
W0712 14:30:26.624156   11254 root.go:314] Error reading config file at /home/ahmedarafa/.minikube/config/config.json: open /home/ahmedarafa/.minikube/config/config.json: no such file or directory
I0712 14:30:26.626487   11254 out.go:352] Setting JSON to false
I0712 14:30:26.676586   11254 start.go:130] hostinfo: {"hostname":"ahmedarafa-VMware-Virtual-Platform","uptime":1419,"bootTime":1752318407,"procs":344,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.11.0-29-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"host","hostId":"64abc7ff-69ac-41e3-8f77-b8995a20e304"}
I0712 14:30:26.677898   11254 start.go:140] virtualization: vbox host
I0712 14:30:26.681317   11254 out.go:177] 😄  minikube v1.36.0 on Ubuntu 24.04
I0712 14:30:26.686389   11254 notify.go:220] Checking for updates...
I0712 14:30:26.686632   11254 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0712 14:30:26.695907   11254 driver.go:404] Setting default libvirt URI to qemu:///system
I0712 14:30:26.871152   11254 docker.go:123] docker version: linux-27.5.1:
I0712 14:30:26.871817   11254 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0712 14:30:27.002509   11254 info.go:266] docker info: {ID:b4b9c521-a2db-496d-b11a-60afd975ed54 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:31 OomKillDisable:false NGoroutines:52 SystemTime:2025-07-12 14:30:26.943738584 +0300 EEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-29-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:3028926464 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ahmedarafa-VMware-Virtual-Platform Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0712 14:30:27.003334   11254 docker.go:318] overlay module found
I0712 14:30:27.006500   11254 out.go:177] ✨  Using the docker driver based on existing profile
I0712 14:30:27.008445   11254 start.go:304] selected driver: docker
I0712 14:30:27.008906   11254 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ahmedarafa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0712 14:30:27.009406   11254 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0712 14:30:27.010241   11254 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0712 14:30:27.152872   11254 info.go:266] docker info: {ID:b4b9c521-a2db-496d-b11a-60afd975ed54 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:31 OomKillDisable:false NGoroutines:52 SystemTime:2025-07-12 14:30:27.094580004 +0300 EEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-29-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:3028926464 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ahmedarafa-VMware-Virtual-Platform Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0712 14:30:27.160131   11254 out.go:201] 
W0712 14:30:27.162677   11254 out.go:270] 🧯  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2888MiB). You may face stability issues.
W0712 14:30:27.162923   11254 out.go:270] 💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'
I0712 14:30:27.164881   11254 out.go:201] 
I0712 14:30:27.172256   11254 cni.go:84] Creating CNI manager for ""
I0712 14:30:27.172736   11254 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0712 14:30:27.173052   11254 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ahmedarafa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0712 14:30:27.177172   11254 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0712 14:30:27.182965   11254 cache.go:121] Beginning downloading kic base image for docker with docker
I0712 14:30:27.187150   11254 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0712 14:30:27.192977   11254 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0712 14:30:27.193261   11254 preload.go:146] Found local preload: /home/ahmedarafa/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0712 14:30:27.193275   11254 cache.go:56] Caching tarball of preloaded images
I0712 14:30:27.194583   11254 preload.go:172] Found /home/ahmedarafa/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0712 14:30:27.194670   11254 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0712 14:30:27.195346   11254 profile.go:143] Saving config to /home/ahmedarafa/.minikube/profiles/minikube/config.json ...
I0712 14:30:27.200929   11254 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0712 14:30:27.381915   11254 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0712 14:30:27.381946   11254 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0712 14:30:27.382194   11254 cache.go:230] Successfully downloaded all kic artifacts
I0712 14:30:27.382660   11254 start.go:360] acquireMachinesLock for minikube: {Name:mk957dc7bcfeb874e682ccc55db6d5b7d88e67c2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0712 14:30:27.383284   11254 start.go:364] duration metric: took 459.044µs to acquireMachinesLock for "minikube"
I0712 14:30:27.383656   11254 start.go:96] Skipping create...Using existing machine configuration
I0712 14:30:27.383675   11254 fix.go:54] fixHost starting: 
I0712 14:30:27.385316   11254 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0712 14:30:27.464507   11254 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0712 14:30:27.464557   11254 fix.go:138] unexpected machine state, will restart: <nil>
I0712 14:30:27.467702   11254 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0712 14:30:27.473210   11254 machine.go:93] provisionDockerMachine start ...
I0712 14:30:27.473375   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:27.556304   11254 main.go:141] libmachine: Using SSH client type: native
I0712 14:30:27.557479   11254 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0712 14:30:27.557498   11254 main.go:141] libmachine: About to run SSH command:
hostname
I0712 14:30:27.875449   11254 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0712 14:30:27.875901   11254 ubuntu.go:169] provisioning hostname "minikube"
I0712 14:30:27.876679   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:27.978341   11254 main.go:141] libmachine: Using SSH client type: native
I0712 14:30:27.979130   11254 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0712 14:30:27.979168   11254 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0712 14:30:28.358927   11254 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0712 14:30:28.359349   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:28.480517   11254 main.go:141] libmachine: Using SSH client type: native
I0712 14:30:28.481556   11254 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0712 14:30:28.481625   11254 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0712 14:30:28.778433   11254 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0712 14:30:28.778500   11254 ubuntu.go:175] set auth options {CertDir:/home/ahmedarafa/.minikube CaCertPath:/home/ahmedarafa/.minikube/certs/ca.pem CaPrivateKeyPath:/home/ahmedarafa/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/ahmedarafa/.minikube/machines/server.pem ServerKeyPath:/home/ahmedarafa/.minikube/machines/server-key.pem ClientKeyPath:/home/ahmedarafa/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/ahmedarafa/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/ahmedarafa/.minikube}
I0712 14:30:28.778571   11254 ubuntu.go:177] setting up certificates
I0712 14:30:28.778623   11254 provision.go:84] configureAuth start
I0712 14:30:28.778955   11254 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0712 14:30:28.875206   11254 provision.go:143] copyHostCerts
I0712 14:30:28.875761   11254 exec_runner.go:144] found /home/ahmedarafa/.minikube/ca.pem, removing ...
I0712 14:30:28.876114   11254 exec_runner.go:203] rm: /home/ahmedarafa/.minikube/ca.pem
I0712 14:30:28.876524   11254 exec_runner.go:151] cp: /home/ahmedarafa/.minikube/certs/ca.pem --> /home/ahmedarafa/.minikube/ca.pem (1090 bytes)
I0712 14:30:28.877532   11254 exec_runner.go:144] found /home/ahmedarafa/.minikube/cert.pem, removing ...
I0712 14:30:28.877555   11254 exec_runner.go:203] rm: /home/ahmedarafa/.minikube/cert.pem
I0712 14:30:28.877700   11254 exec_runner.go:151] cp: /home/ahmedarafa/.minikube/certs/cert.pem --> /home/ahmedarafa/.minikube/cert.pem (1131 bytes)
I0712 14:30:28.878449   11254 exec_runner.go:144] found /home/ahmedarafa/.minikube/key.pem, removing ...
I0712 14:30:28.878467   11254 exec_runner.go:203] rm: /home/ahmedarafa/.minikube/key.pem
I0712 14:30:28.878585   11254 exec_runner.go:151] cp: /home/ahmedarafa/.minikube/certs/key.pem --> /home/ahmedarafa/.minikube/key.pem (1675 bytes)
I0712 14:30:28.879073   11254 provision.go:117] generating server cert: /home/ahmedarafa/.minikube/machines/server.pem ca-key=/home/ahmedarafa/.minikube/certs/ca.pem private-key=/home/ahmedarafa/.minikube/certs/ca-key.pem org=ahmedarafa.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0712 14:30:29.340002   11254 provision.go:177] copyRemoteCerts
I0712 14:30:29.341430   11254 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0712 14:30:29.341847   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:29.436686   11254 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ahmedarafa/.minikube/machines/minikube/id_rsa Username:docker}
I0712 14:30:29.629437   11254 ssh_runner.go:362] scp /home/ahmedarafa/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0712 14:30:29.764662   11254 ssh_runner.go:362] scp /home/ahmedarafa/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0712 14:30:29.928063   11254 ssh_runner.go:362] scp /home/ahmedarafa/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0712 14:30:30.072310   11254 provision.go:87] duration metric: took 1.293538363s to configureAuth
I0712 14:30:30.072346   11254 ubuntu.go:193] setting minikube options for container-runtime
I0712 14:30:30.072706   11254 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0712 14:30:30.073012   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:30.145733   11254 main.go:141] libmachine: Using SSH client type: native
I0712 14:30:30.146463   11254 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0712 14:30:30.146481   11254 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0712 14:30:30.441099   11254 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0712 14:30:30.441126   11254 ubuntu.go:71] root file system type: overlay
I0712 14:30:30.441876   11254 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0712 14:30:30.442037   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:30.525664   11254 main.go:141] libmachine: Using SSH client type: native
I0712 14:30:30.526464   11254 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0712 14:30:30.526698   11254 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0712 14:30:30.882164   11254 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0712 14:30:30.882356   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:30.979534   11254 main.go:141] libmachine: Using SSH client type: native
I0712 14:30:30.980446   11254 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0712 14:30:30.980485   11254 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0712 14:30:31.418445   11254 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0712 14:30:31.418485   11254 machine.go:96] duration metric: took 3.945256148s to provisionDockerMachine
I0712 14:30:31.418512   11254 start.go:293] postStartSetup for "minikube" (driver="docker")
I0712 14:30:31.418540   11254 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0712 14:30:31.418746   11254 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0712 14:30:31.418894   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:31.533634   11254 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ahmedarafa/.minikube/machines/minikube/id_rsa Username:docker}
I0712 14:30:31.803292   11254 ssh_runner.go:195] Run: cat /etc/os-release
I0712 14:30:31.823718   11254 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0712 14:30:31.823838   11254 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0712 14:30:31.823868   11254 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0712 14:30:31.823910   11254 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0712 14:30:31.823940   11254 filesync.go:126] Scanning /home/ahmedarafa/.minikube/addons for local assets ...
I0712 14:30:31.824107   11254 filesync.go:126] Scanning /home/ahmedarafa/.minikube/files for local assets ...
I0712 14:30:31.824204   11254 start.go:296] duration metric: took 405.669905ms for postStartSetup
I0712 14:30:31.824412   11254 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0712 14:30:31.824489   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:31.922204   11254 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ahmedarafa/.minikube/machines/minikube/id_rsa Username:docker}
I0712 14:30:32.126723   11254 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0712 14:30:32.167054   11254 fix.go:56] duration metric: took 4.78336324s for fixHost
I0712 14:30:32.167099   11254 start.go:83] releasing machines lock for "minikube", held for 4.783792016s
I0712 14:30:32.167282   11254 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0712 14:30:32.250005   11254 ssh_runner.go:195] Run: cat /version.json
I0712 14:30:32.250091   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:32.250962   11254 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0712 14:30:32.251087   11254 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0712 14:30:32.399677   11254 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ahmedarafa/.minikube/machines/minikube/id_rsa Username:docker}
I0712 14:30:32.404542   11254 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ahmedarafa/.minikube/machines/minikube/id_rsa Username:docker}
I0712 14:30:32.595029   11254 ssh_runner.go:195] Run: systemctl --version
I0712 14:30:33.368247   11254 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0712 14:30:33.368536   11254 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.11753927s)
I0712 14:30:33.404666   11254 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0712 14:30:33.569346   11254 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0712 14:30:33.569506   11254 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0712 14:30:33.624843   11254 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0712 14:30:33.624886   11254 start.go:495] detecting cgroup driver to use...
I0712 14:30:33.624967   11254 detect.go:190] detected "systemd" cgroup driver on host os
I0712 14:30:33.626201   11254 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0712 14:30:33.737480   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0712 14:30:33.812681   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0712 14:30:33.881199   11254 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0712 14:30:33.881300   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0712 14:30:33.979621   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0712 14:30:34.059075   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0712 14:30:34.136963   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0712 14:30:34.209435   11254 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0712 14:30:34.289107   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0712 14:30:34.349601   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0712 14:30:34.435594   11254 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0712 14:30:34.498475   11254 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0712 14:30:34.564140   11254 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0712 14:30:34.583606   11254 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0712 14:30:34.827036   11254 ssh_runner.go:195] Run: sudo systemctl restart containerd


==> Docker <==
Jul 12 11:30:35 minikube dockerd[1268]: time="2025-07-12T11:30:35.391831447Z" level=info msg="ignoring event" container=0912d768a87a794e07be2d1dcfb01787a1ab476442ecd4460e5779264ccf6893 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 12 11:30:37 minikube cri-dockerd[1565]: time="2025-07-12T11:30:37Z" level=error msg="error getting RW layer size for container ID 'f4fa9acb24bb26299bb38c1e5d5f7c9e261d9fd2f5581df6525dd555ae81ec36': Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Jul 12 11:30:37 minikube cri-dockerd[1565]: time="2025-07-12T11:30:37Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'f4fa9acb24bb26299bb38c1e5d5f7c9e261d9fd2f5581df6525dd555ae81ec36'"
Jul 12 11:30:40 minikube dockerd[1268]: time="2025-07-12T11:30:40.240127897Z" level=info msg="ignoring event" container=046e1821843962f2da75da09c0a17379b16772b62a0b02b48aaa8ce55d610f27 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 12 11:30:42 minikube dockerd[1268]: time="2025-07-12T11:30:42.134655180Z" level=info msg="ignoring event" container=9cc34f9ec93a16a515858a3c13393fc76cc969d55cd3c2d02c39401052f5318b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 12 11:30:45 minikube dockerd[1268]: time="2025-07-12T11:30:45.135198920Z" level=info msg="Container failed to exit within 10s of signal 15 - using the force" container=5f1fb9ac9538a013bcfd377e62789ee21181759878929619c373bf1dd963c4f6
Jul 12 11:30:45 minikube dockerd[1268]: time="2025-07-12T11:30:45.329408741Z" level=info msg="ignoring event" container=5f1fb9ac9538a013bcfd377e62789ee21181759878929619c373bf1dd963c4f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 12 11:30:45 minikube dockerd[1268]: time="2025-07-12T11:30:45.432768340Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jul 12 11:30:45 minikube dockerd[1268]: time="2025-07-12T11:30:45.434393088Z" level=info msg="Daemon shutdown complete"
Jul 12 11:30:45 minikube systemd[1]: docker.service: Deactivated successfully.
Jul 12 11:30:45 minikube systemd[1]: Stopped Docker Application Container Engine.
Jul 12 11:30:45 minikube systemd[1]: docker.service: Consumed 11.154s CPU time.
Jul 12 11:30:45 minikube systemd[1]: Starting Docker Application Container Engine...
Jul 12 11:30:46 minikube dockerd[4742]: time="2025-07-12T11:30:46.124116105Z" level=info msg="Starting up"
Jul 12 11:30:46 minikube dockerd[4742]: time="2025-07-12T11:30:46.130349581Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jul 12 11:30:46 minikube dockerd[4742]: time="2025-07-12T11:30:46.233551044Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jul 12 11:30:46 minikube dockerd[4742]: time="2025-07-12T11:30:46.272764347Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 12 11:30:46 minikube dockerd[4742]: time="2025-07-12T11:30:46.504107881Z" level=info msg="Loading containers: start."
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.557023337Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 6c9cd70823c0559a6a8b29a4b2e8b974bb506187f3677115d4e0c609a90c1aef], retrying...."
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.674550204Z" level=info msg="Loading containers: done."
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.696222625Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.696372499Z" level=info msg="Initializing buildkit"
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.742042348Z" level=info msg="Completed buildkit initialization"
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.748211320Z" level=info msg="Daemon has completed initialization"
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.748322273Z" level=info msg="API listen on /var/run/docker.sock"
Jul 12 11:30:48 minikube dockerd[4742]: time="2025-07-12T11:30:48.750360793Z" level=info msg="API listen on [::]:2376"
Jul 12 11:30:48 minikube systemd[1]: Started Docker Application Container Engine.
Jul 12 11:30:48 minikube cri-dockerd[1565]: time="2025-07-12T11:30:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-t42pm_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"49745ec08438e7660fcc3c6201e980e7f4b431d65577cd37ec9fbb57c31deb3d\""
Jul 12 11:30:49 minikube cri-dockerd[1565]: time="2025-07-12T11:30:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/94bc8006d860b10c22a75b8cda5a20765168b1d7a680a415086b98285604c8c6/resolv.conf as [nameserver 192.168.49.1 search localdomain options trust-ad ndots:0 edns0]"
Jul 12 11:30:49 minikube cri-dockerd[1565]: time="2025-07-12T11:30:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/be4c77d141f26dbab2f4e85273b0b74712dae046faef34a3482bc4082d32a23e/resolv.conf as [nameserver 192.168.49.1 search localdomain options trust-ad ndots:0 edns0]"
Jul 12 11:30:50 minikube cri-dockerd[1565]: time="2025-07-12T11:30:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a91c1d47db89cd579c44279b02aa6ae87f2ea9a326f203a73afabd9da0ec52e/resolv.conf as [nameserver 192.168.49.1 search localdomain options edns0 trust-ad ndots:0]"
Jul 12 11:30:50 minikube cri-dockerd[1565]: time="2025-07-12T11:30:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7931ce28e4f5b6994b539dd4943921a60c1a569de0059ca9604afc7043baffa2/resolv.conf as [nameserver 192.168.49.1 search localdomain options edns0 trust-ad ndots:0]"
Jul 12 11:30:50 minikube cri-dockerd[1565]: time="2025-07-12T11:30:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d3f4d637fa8c9e2ae971b7a9db3e5b8794b0ac6f5ba1fee760b2c8761bda77ef/resolv.conf as [nameserver 192.168.49.1 search localdomain options edns0 trust-ad ndots:0]"
Jul 12 11:30:50 minikube cri-dockerd[1565]: time="2025-07-12T11:30:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d25c9271969fc08e53d7bf21e3a509e2d23855dc3b601de3b8fc8d0c2f38390b/resolv.conf as [nameserver 192.168.49.1 search localdomain options edns0 trust-ad ndots:0]"
Jul 12 11:30:50 minikube cri-dockerd[1565]: time="2025-07-12T11:30:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/119227a075c9035837921bdfb7fccb888279e1819d2558bf0f1ee968757ca57e/resolv.conf as [nameserver 192.168.49.1 search localdomain options edns0 trust-ad ndots:0]"
Jul 12 11:42:19 minikube cri-dockerd[1565]: time="2025-07-12T11:42:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/40dff6bf5485e34a80e5b4e01419a252da45f6e74f513a2b7868087552482bd7/resolv.conf as [nameserver 10.96.0.10 search vote.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jul 12 11:42:34 minikube cri-dockerd[1565]: time="2025-07-12T11:42:34Z" level=info msg="Pulling image redis:latest: 574052bfcd56: Downloading [===================>                               ]  8.342MB/21.27MB"
Jul 12 11:42:44 minikube cri-dockerd[1565]: time="2025-07-12T11:42:44Z" level=info msg="Pulling image redis:latest: 3da95a905ed5: Extracting [=========================================>         ]  23.59MB/28.23MB"
Jul 12 11:42:52 minikube cri-dockerd[1565]: time="2025-07-12T11:42:52Z" level=info msg="Stop pulling image redis:latest: Status: Downloaded newer image for redis:latest"
Jul 12 11:45:36 minikube cri-dockerd[1565]: time="2025-07-12T11:45:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3c04d5c212a3aade4238cc0bfc97a88d7246e27fe9ad3dd19d2f0a400dee4884/resolv.conf as [nameserver 10.96.0.10 search vote.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jul 12 11:45:50 minikube cri-dockerd[1565]: time="2025-07-12T11:45:50Z" level=info msg="Pulling image postgres:latest: e9068395fab4: Downloading [=========================================>         ]  6.732MB/8.066MB"
Jul 12 11:46:00 minikube cri-dockerd[1565]: time="2025-07-12T11:46:00Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Downloading [===============>                                   ]  35.68MB/112.8MB"
Jul 12 11:46:10 minikube cri-dockerd[1565]: time="2025-07-12T11:46:10Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Downloading [===============================>                   ]  70.82MB/112.8MB"
Jul 12 11:46:20 minikube cri-dockerd[1565]: time="2025-07-12T11:46:20Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Downloading [==============================================>    ]  105.4MB/112.8MB"
Jul 12 11:46:30 minikube cri-dockerd[1565]: time="2025-07-12T11:46:30Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Extracting [======================>                            ]  51.81MB/112.8MB"
Jul 12 11:46:40 minikube cri-dockerd[1565]: time="2025-07-12T11:46:40Z" level=info msg="Pulling image postgres:latest: 1eb73c80cbec: Extracting [============================================>      ]  100.3MB/112.8MB"
Jul 12 11:46:46 minikube cri-dockerd[1565]: time="2025-07-12T11:46:46Z" level=info msg="Stop pulling image postgres:latest: Status: Downloaded newer image for postgres:latest"
Jul 12 11:47:58 minikube cri-dockerd[1565]: time="2025-07-12T11:47:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dd383981e13fcb265f8730de0d9a5618bd1a0556097b295337e45e2f6dd7094a/resolv.conf as [nameserver 10.96.0.10 search vote.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jul 12 11:47:58 minikube cri-dockerd[1565]: time="2025-07-12T11:47:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4d0bcb7d2b4cd4746220fa37534be54393979cc5d250110adefd63345a2b5f29/resolv.conf as [nameserver 10.96.0.10 search vote.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jul 12 11:48:13 minikube cri-dockerd[1565]: time="2025-07-12T11:48:13Z" level=info msg="Pulling image docker/example-voting-app-vote:latest: 3df19ea857b8: Downloading [====================================>              ]  3.447MB/4.712MB"
Jul 12 11:48:23 minikube cri-dockerd[1565]: time="2025-07-12T11:48:23Z" level=info msg="Pulling image docker/example-voting-app-vote:latest: 85c091939eb6: Extracting [===========================================>       ]  18.58MB/21.34MB"
Jul 12 11:48:29 minikube cri-dockerd[1565]: time="2025-07-12T11:48:29Z" level=info msg="Stop pulling image docker/example-voting-app-vote:latest: Status: Downloaded newer image for docker/example-voting-app-vote:latest"
Jul 12 11:48:32 minikube cri-dockerd[1565]: time="2025-07-12T11:48:32Z" level=info msg="Stop pulling image docker/example-voting-app-vote:latest: Status: Image is up to date for docker/example-voting-app-vote:latest"
Jul 12 11:52:06 minikube cri-dockerd[1565]: time="2025-07-12T11:52:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8fde1a9889dc113d70b27de3e293a7f90d9b31adbca18d5fd96dcc679877457e/resolv.conf as [nameserver 10.96.0.10 search vote.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jul 12 11:52:06 minikube cri-dockerd[1565]: time="2025-07-12T11:52:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/858d31813f03a10024f75e23934edf080af6a345e01b6d7805f1a1ecefbd85cc/resolv.conf as [nameserver 10.96.0.10 search vote.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jul 12 11:52:21 minikube cri-dockerd[1565]: time="2025-07-12T11:52:21Z" level=info msg="Pulling image docker/example-voting-app-result:latest: 1bb8eaf3d643: Downloading [===================================>               ]  13.15MB/18.53MB"
Jul 12 11:52:31 minikube cri-dockerd[1565]: time="2025-07-12T11:52:31Z" level=info msg="Pulling image docker/example-voting-app-result:latest: 4a41b3205790: Downloading [=======>                                           ]  245.2kB/1.571MB"
Jul 12 11:52:41 minikube cri-dockerd[1565]: time="2025-07-12T11:52:41Z" level=info msg="Pulling image docker/example-voting-app-result:latest: 8b87079b7a06: Downloading [=============================================>     ]  47.18MB/51.36MB"
Jul 12 11:52:51 minikube cri-dockerd[1565]: time="2025-07-12T11:52:51Z" level=info msg="Pulling image docker/example-voting-app-result:latest: 8b87079b7a06: Extracting [============================================>      ]  46.14MB/51.36MB"
Jul 12 11:53:01 minikube cri-dockerd[1565]: time="2025-07-12T11:53:01Z" level=info msg="Pulling image docker/example-voting-app-result:latest: 1bb8eaf3d643: Extracting [=================================================> ]  18.48MB/18.53MB"


==> container status <==
CONTAINER           IMAGE                                                                                                    CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d2253a8d47d9d       docker/example-voting-app-vote@sha256:d1ac598ad5f1dc72dfb8c2100c38b7d685f25bf736eee0edc39befce69cb69a0   4 minutes ago       Running             vote                      0                   4d0bcb7d2b4cd       vote-app-79b7b95c55-6sl6k
14b20a9fa386b       docker/example-voting-app-vote@sha256:d1ac598ad5f1dc72dfb8c2100c38b7d685f25bf736eee0edc39befce69cb69a0   4 minutes ago       Running             vote                      0                   dd383981e13fc       vote-app-79b7b95c55-m8dvw
da627771519d1       postgres@sha256:3962158596daaef3682838cc8eb0e719ad1ce520f88e34596ce8d5de1b6330a1                         6 minutes ago       Running             postgres                  0                   3c04d5c212a3a       postgres-9b97bcfdc-7qx5d
22d27db52c123       redis@sha256:a1e0a3b3a6cedd74d8ba44805b2497b93851a296f08a44962fedc03f1c490b47                            10 minutes ago      Running             redis                     0                   40dff6bf5485e       redis-7676fddfbb-7pcdg
edbac3b7efc77       b79c189b052cd                                                                                            22 minutes ago      Running             kube-proxy                1                   d25c9271969fc       kube-proxy-wcbsz
9efa790342215       1cf5f116067c6                                                                                            22 minutes ago      Running             coredns                   1                   7931ce28e4f5b       coredns-674b8bbfcf-t42pm
260235fe70ddd       6e38f40d628db                                                                                            22 minutes ago      Running             storage-provisioner       2                   119227a075c90       storage-provisioner
6c9e9cec26202       ef43894fa110c                                                                                            22 minutes ago      Running             kube-controller-manager   1                   d3f4d637fa8c9       kube-controller-manager-minikube
27fb3c386d667       398c985c0d950                                                                                            22 minutes ago      Running             kube-scheduler            1                   5a91c1d47db89       kube-scheduler-minikube
29d49408f6e35       499038711c081                                                                                            22 minutes ago      Running             etcd                      1                   94bc8006d860b       etcd-minikube
356ddc14baf5e       c6ab243b29f82                                                                                            22 minutes ago      Running             kube-apiserver            1                   be4c77d141f26       kube-apiserver-minikube
ed345f136c276       6e38f40d628db                                                                                            26 minutes ago      Exited              storage-provisioner       1                   6244ea8bc524b       storage-provisioner
046e182184396       1cf5f116067c6                                                                                            26 minutes ago      Exited              coredns                   0                   49745ec08438e       coredns-674b8bbfcf-t42pm
c13ed318eb579       b79c189b052cd                                                                                            26 minutes ago      Exited              kube-proxy                0                   83e1f7a465177       kube-proxy-wcbsz
9cc34f9ec93a1       499038711c081                                                                                            27 minutes ago      Exited              etcd                      0                   980892f746b80       etcd-minikube
3ec17cdcb3b5e       398c985c0d950                                                                                            27 minutes ago      Exited              kube-scheduler            0                   f4ecd20aa4ebe       kube-scheduler-minikube
5f1fb9ac9538a       c6ab243b29f82                                                                                            27 minutes ago      Exited              kube-apiserver            0                   0912d768a87a7       kube-apiserver-minikube
162557ea10aa5       ef43894fa110c                                                                                            27 minutes ago      Exited              kube-controller-manager   0                   2389728a479a3       kube-controller-manager-minikube


==> coredns [046e18218439] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:39091 - 13953 "HINFO IN 6251466349445933757.3170063601916175586. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.073677327s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [9efa79034221] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:51410 - 29117 "HINFO IN 4256941915045282331.3687202215767586170. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.085957252s
[INFO] 10.244.0.8:48891 - 56788 "A IN redis.vote.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.22658596s
[INFO] 10.244.0.8:34393 - 12832 "A IN redis.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.01431523s
[INFO] 10.244.0.8:48891 - 57398 "AAAA IN redis.vote.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.282654609s
[INFO] 10.244.0.8:34393 - 13887 "AAAA IN redis.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.009231548s
[INFO] 10.244.0.8:33720 - 41529 "AAAA IN redis.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.010403174s
[INFO] 10.244.0.8:33720 - 40796 "A IN redis.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.016125639s
[INFO] 10.244.0.8:39880 - 58755 "AAAA IN redis.localdomain. udp 35 false 512" NXDOMAIN qr,rd,ra 110 0.130153915s
[INFO] 10.244.0.8:39880 - 57896 "A IN redis.localdomain. udp 35 false 512" NXDOMAIN qr,rd,ra 110 0.12985979s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,rd 23 0.012656926s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000530555s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000348828s
[INFO] 10.244.0.8:48585 - 31278 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000292677s
[INFO] 10.244.0.8:48585 - 31278 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,rd 23 0.032007865s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000334589s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000261866s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.002462178s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000237173s
[INFO] 10.244.0.8:48585 - 31278 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.00087606s
[INFO] 10.244.0.8:48585 - 30711 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000198559s
[INFO] 10.244.0.8:48585 - 31278 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000482438s
[INFO] 10.244.0.8:48639 - 35900 "A IN redis.vote.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.002038662s
[INFO] 10.244.0.8:48639 - 36816 "AAAA IN redis.vote.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000555242s
[INFO] 10.244.0.8:56160 - 31200 "A IN redis.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000355234s
[INFO] 10.244.0.8:56160 - 31770 "AAAA IN redis.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000690319s
[INFO] 10.244.0.8:36352 - 11840 "AAAA IN redis.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000407935s
[INFO] 10.244.0.8:36352 - 11229 "A IN redis.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.001550767s
[INFO] 10.244.0.8:56774 - 38331 "A IN redis.localdomain. udp 35 false 512" NXDOMAIN qr,rd,ra 110 0.07422541s
[INFO] 10.244.0.8:56774 - 39131 "AAAA IN redis.localdomain. udp 35 false 512" NXDOMAIN qr,rd,ra 110 0.076999404s
[INFO] 10.244.0.8:44512 - 5616 "A IN redis. udp 23 false 512" SERVFAIL qr,rd 23 0.003984354s
[INFO] 10.244.0.8:44512 - 5616 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.002274337s
[INFO] 10.244.0.8:44512 - 6330 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,rd 23 0.015513349s
[INFO] 10.244.0.8:44512 - 5616 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.003489838s
[INFO] 10.244.0.8:44512 - 6330 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.003266255s
[INFO] 10.244.0.8:44512 - 5616 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.001854112s
[INFO] 10.244.0.8:44512 - 5616 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000368915s
[INFO] 10.244.0.8:44512 - 6330 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000496652s
[INFO] 10.244.0.8:44512 - 6330 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.000385831s
[INFO] 10.244.0.8:44512 - 6330 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.00040621s
[INFO] 10.244.0.8:44512 - 6330 "AAAA IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.00683791s
[INFO] 10.244.0.8:44512 - 5616 "A IN redis. udp 23 false 512" SERVFAIL qr,aa,rd 23 0.009160126s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_12T14_26_15_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 12 Jul 2025 11:26:11 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 12 Jul 2025 11:53:07 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 12 Jul 2025 11:48:53 +0000   Sat, 12 Jul 2025 11:26:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 12 Jul 2025 11:48:53 +0000   Sat, 12 Jul 2025 11:26:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 12 Jul 2025 11:48:53 +0000   Sat, 12 Jul 2025 11:26:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 12 Jul 2025 11:48:53 +0000   Sat, 12 Jul 2025 11:26:15 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  20463184Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2957936Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  20463184Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2957936Ki
  pods:               110
System Info:
  Machine ID:                 fb2e41cf112e44478d145784021133ca
  System UUID:                4fb7836f-3a7d-4712-bca1-9ffdeb8eb9a9
  Boot ID:                    32d1c168-75e7-486f-9a60-1eecfcc688cd
  Kernel Version:             6.11.0-29-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-674b8bbfcf-t42pm            100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     26m
  kube-system                 etcd-minikube                       100m (5%)     0 (0%)      100Mi (3%)       0 (0%)         26m
  kube-system                 kube-apiserver-minikube             250m (12%)    0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 kube-controller-manager-minikube    200m (10%)    0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 kube-proxy-wcbsz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 kube-scheduler-minikube             100m (5%)     0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m
  vote                        postgres-9b97bcfdc-7qx5d            500m (25%)    1 (50%)     256Mi (8%)       512Mi (17%)    7m35s
  vote                        redis-7676fddfbb-7pcdg              200m (10%)    500m (25%)  128Mi (4%)       256Mi (8%)     10m
  vote                        result-app-7cc59d596-7p2fc          100m (5%)     300m (15%)  64Mi (2%)        128Mi (4%)     65s
  vote                        result-app-7cc59d596-wmtm4          100m (5%)     300m (15%)  64Mi (2%)        128Mi (4%)     65s
  vote                        vote-app-79b7b95c55-6sl6k           100m (5%)     300m (15%)  64Mi (2%)        128Mi (4%)     5m13s
  vote                        vote-app-79b7b95c55-m8dvw           100m (5%)     300m (15%)  64Mi (2%)        128Mi (4%)     5m13s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1850m (92%)  2700m (135%)
  memory             810Mi (28%)  1450Mi (50%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 26m                kube-proxy       
  Normal  Starting                 22m                kube-proxy       
  Normal  NodeHasSufficientMemory  27m (x8 over 27m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    27m (x8 over 27m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     27m (x7 over 27m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  27m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 26m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                26m                kubelet          Node minikube status is now: NodeReady
  Normal  NodeHasSufficientMemory  26m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    26m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     26m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           26m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  RegisteredNode           22m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jul12 11:06] core: CPUID marked event: 'cpu cycles' unavailable
[  +0.000004] core: CPUID marked event: 'instructions' unavailable
[  +0.000004] core: CPUID marked event: 'bus cycles' unavailable
[  +0.000003] core: CPUID marked event: 'cache references' unavailable
[  +0.000003] core: CPUID marked event: 'cache misses' unavailable
[  +0.000003] core: CPUID marked event: 'branch instructions' unavailable
[  +0.000003] core: CPUID marked event: 'branch misses' unavailable
[  +5.828220] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[Jul12 11:07] sd 2:0:0:0: [sda] Assuming drive cache: write through
[ +13.809186] kauditd_printk_skb: 126 callbacks suppressed
[  +2.595730] workqueue: pcpu_balance_workfn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +1.524993] workqueue: pcpu_balance_workfn hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[  +3.044333] kauditd_printk_skb: 5 callbacks suppressed
[ +10.788232] vboxdrv: loading out-of-tree module taints kernel.
[  +0.415215] VBoxNetFlt: Successfully started.
[  +0.198632] VBoxNetAdp: Successfully started.
[  +1.549665] workqueue: pcpu_balance_workfn hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[  +7.102880] piix4_smbus 0000:00:07.3: SMBus Host Controller not enabled!
[Jul12 11:08] kauditd_printk_skb: 7 callbacks suppressed
[Jul12 11:09] workqueue: pcpu_balance_workfn hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[ +23.969691] /dev/sr1: Can't open blockdev
[  +0.003476] /dev/sr0: Can't open blockdev
[Jul12 11:22] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Jul12 11:24] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[ +33.748072] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[  +3.900342] workqueue: e1000_watchdog [e1000] hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Jul12 11:26] workqueue: psi_avgs_work hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +0.064760] workqueue: psi_avgs_work hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[Jul12 11:37] sched: RT throttling activated
[Jul12 11:39] hrtimer: interrupt took 4926582 ns
[Jul12 11:46] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[Jul12 11:49] workqueue: psi_avgs_work hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND


==> etcd [29d49408f6e3] <==
{"level":"info","ts":"2025-07-12T11:30:50.722430Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-07-12T11:30:50.722478Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-12T11:30:50.723560Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-07-12T11:30:50.723956Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-07-12T11:30:50.735008Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"10.484243ms"}
{"level":"info","ts":"2025-07-12T11:30:50.752624Z","caller":"etcdserver/server.go:534","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-07-12T11:30:50.768792Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":709}
{"level":"info","ts":"2025-07-12T11:30:50.768984Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-07-12T11:30:50.769018Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-07-12T11:30:50.769043Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 709, applied: 0, lastindex: 709, lastterm: 2]"}
{"level":"warn","ts":"2025-07-12T11:30:50.770622Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-07-12T11:30:50.789351Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":648}
{"level":"info","ts":"2025-07-12T11:30:50.789422Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":709}
{"level":"info","ts":"2025-07-12T11:30:50.790750Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-07-12T11:30:50.793278Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-07-12T11:30:50.793956Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-07-12T11:30:50.794849Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-07-12T11:30:50.795269Z","caller":"etcdserver/server.go:775","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-07-12T11:30:50.795494Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-12T11:30:50.795563Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-12T11:30:50.795573Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-12T11:30:50.795827Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-07-12T11:30:50.795877Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-12T11:30:50.797348Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-12T11:30:50.798059Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-12T11:30:50.800531Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-12T11:30:50.809971Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-12T11:30:50.811405Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-12T11:30:50.813174Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-12T11:30:50.813407Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-12T11:30:50.813759Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-12T11:30:52.170032Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-07-12T11:30:52.170138Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-07-12T11:30:52.170201Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-12T11:30:52.174898Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-07-12T11:30:52.178045Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-07-12T11:30:52.178175Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-07-12T11:30:52.178200Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-07-12T11:30:52.195110Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-12T11:30:52.194579Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-12T11:30:52.199331Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-12T11:30:52.203740Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-12T11:30:52.205945Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-12T11:30:52.207654Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-12T11:30:52.215550Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-12T11:30:52.251383Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-12T11:30:52.416365Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-12T11:40:55.422218Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":975}
{"level":"info","ts":"2025-07-12T11:40:55.713177Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":975,"took":"290.324681ms","hash":2977681961,"current-db-size-bytes":2654208,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":2654208,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2025-07-12T11:40:55.713281Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2977681961,"revision":975,"compact-revision":-1}
{"level":"info","ts":"2025-07-12T11:45:55.437079Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1217}
{"level":"info","ts":"2025-07-12T11:45:55.449139Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1217,"took":"11.642941ms","hash":2825041447,"current-db-size-bytes":2654208,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1818624,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-12T11:45:55.449382Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2825041447,"revision":1217,"compact-revision":975}
{"level":"info","ts":"2025-07-12T11:46:52.288350Z","caller":"traceutil/trace.go:171","msg":"trace[359902783] transaction","detail":"{read_only:false; response_revision:1556; number_of_response:1; }","duration":"124.979887ms","start":"2025-07-12T11:46:52.050542Z","end":"2025-07-12T11:46:52.175522Z","steps":["trace[359902783] 'process raft request'  (duration: 46.349569ms)","trace[359902783] 'compare'  (duration: 68.205074ms)"],"step_count":2}
{"level":"info","ts":"2025-07-12T11:49:21.904262Z","caller":"traceutil/trace.go:171","msg":"trace[1271035007] transaction","detail":"{read_only:false; response_revision:1720; number_of_response:1; }","duration":"101.732618ms","start":"2025-07-12T11:49:21.724672Z","end":"2025-07-12T11:49:21.826404Z","steps":["trace[1271035007] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-node-lease/minikube; req_size:518; } (duration: 93.849174ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:49:42.830393Z","caller":"traceutil/trace.go:171","msg":"trace[714731406] transaction","detail":"{read_only:false; response_revision:1736; number_of_response:1; }","duration":"126.454772ms","start":"2025-07-12T11:49:42.703850Z","end":"2025-07-12T11:49:42.830304Z","steps":["trace[714731406] 'process raft request'  (duration: 42.324933ms)","trace[714731406] 'compare'  (duration: 69.951327ms)","trace[714731406] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-node-lease/minikube; req_size:518; } (duration: 13.648496ms)"],"step_count":3}
{"level":"info","ts":"2025-07-12T11:50:55.473801Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1503}
{"level":"info","ts":"2025-07-12T11:50:55.901968Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1503,"took":"417.632895ms","hash":1566983642,"current-db-size-bytes":2654208,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":2101248,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-07-12T11:50:55.902180Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1566983642,"revision":1503,"compact-revision":1217}
{"level":"info","ts":"2025-07-12T11:52:16.383594Z","caller":"traceutil/trace.go:171","msg":"trace[1909411747] transaction","detail":"{read_only:false; response_revision:1883; number_of_response:1; }","duration":"115.229874ms","start":"2025-07-12T11:52:16.242769Z","end":"2025-07-12T11:52:16.357999Z","steps":["trace[1909411747] 'process raft request'  (duration: 82.616188ms)"],"step_count":1}


==> etcd [9cc34f9ec93a] <==
{"level":"info","ts":"2025-07-12T11:25:55.080134Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-12T11:25:55.082205Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-07-12T11:25:55.083412Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-12T11:25:55.099124Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-12T11:25:55.101233Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-12T11:25:55.101370Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-12T11:25:55.118443Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-12T11:25:55.121070Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-12T11:25:55.848206Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-07-12T11:25:55.848406Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-07-12T11:25:55.849733Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-07-12T11:25:55.849902Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-07-12T11:25:55.850687Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-12T11:25:55.850742Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-07-12T11:25:55.850774Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-12T11:25:55.868938Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-12T11:25:55.872233Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-12T11:25:55.874895Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-12T11:25:55.878768Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-12T11:25:55.880894Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-12T11:25:55.885409Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-12T11:25:55.893986Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-12T11:25:55.894098Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-12T11:25:55.895218Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-12T11:25:55.900864Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-12T11:25:55.911513Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-12T11:25:55.924242Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-12T11:25:55.987467Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-12T11:26:17.320594Z","caller":"traceutil/trace.go:171","msg":"trace[1013744302] transaction","detail":"{read_only:false; response_revision:329; number_of_response:1; }","duration":"249.285006ms","start":"2025-07-12T11:26:17.071271Z","end":"2025-07-12T11:26:17.320556Z","steps":["trace[1013744302] 'process raft request'  (duration: 248.23147ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:26:19.306458Z","caller":"traceutil/trace.go:171","msg":"trace[1158922869] transaction","detail":"{read_only:false; response_revision:360; number_of_response:1; }","duration":"115.259148ms","start":"2025-07-12T11:26:19.191157Z","end":"2025-07-12T11:26:19.306416Z","steps":["trace[1158922869] 'process raft request'  (duration: 115.065437ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-12T11:26:19.415952Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-12T11:26:19.066763Z","time spent":"318.188697ms","remote":"127.0.0.1:56190","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1743,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/configmaps/kube-node-lease/kube-root-ca.crt\" mod_revision:0 > success:<request_put:<key:\"/registry/configmaps/kube-node-lease/kube-root-ca.crt\" value_size:1682 >> failure:<>"}
{"level":"info","ts":"2025-07-12T11:26:19.648736Z","caller":"traceutil/trace.go:171","msg":"trace[1271570863] transaction","detail":"{read_only:false; response_revision:362; number_of_response:1; }","duration":"117.98051ms","start":"2025-07-12T11:26:19.530040Z","end":"2025-07-12T11:26:19.648020Z","steps":["trace[1271570863] 'process raft request'  (duration: 92.383323ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:26:19.664035Z","caller":"traceutil/trace.go:171","msg":"trace[487177305] transaction","detail":"{read_only:false; response_revision:363; number_of_response:1; }","duration":"109.713939ms","start":"2025-07-12T11:26:19.554286Z","end":"2025-07-12T11:26:19.664000Z","steps":["trace[487177305] 'process raft request'  (duration: 90.881821ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-12T11:26:19.675101Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.437098ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/kube-proxy\" limit:1 ","response":"range_response_count:1 size:185"}
{"level":"info","ts":"2025-07-12T11:26:19.680684Z","caller":"traceutil/trace.go:171","msg":"trace[316709011] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/kube-proxy; range_end:; response_count:1; response_revision:363; }","duration":"144.587668ms","start":"2025-07-12T11:26:19.536073Z","end":"2025-07-12T11:26:19.680661Z","steps":["trace[316709011] 'agreement among raft nodes before linearized reading'  (duration: 138.786413ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:26:19.658980Z","caller":"traceutil/trace.go:171","msg":"trace[77918707] linearizableReadLoop","detail":"{readStateIndex:370; appliedIndex:369; }","duration":"116.419489ms","start":"2025-07-12T11:26:19.536612Z","end":"2025-07-12T11:26:19.653031Z","steps":["trace[77918707] 'read index received'  (duration: 85.692084ms)","trace[77918707] 'applied index is now lower than readState.Index'  (duration: 30.724139ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-12T11:26:19.840479Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"176.438023ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:4486"}
{"level":"info","ts":"2025-07-12T11:26:19.840643Z","caller":"traceutil/trace.go:171","msg":"trace[1446621467] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:363; }","duration":"176.692401ms","start":"2025-07-12T11:26:19.663913Z","end":"2025-07-12T11:26:19.840605Z","steps":["trace[1446621467] 'agreement among raft nodes before linearized reading'  (duration: 67.293067ms)","trace[1446621467] 'range keys from in-memory index tree'  (duration: 109.135879ms)"],"step_count":2}
{"level":"info","ts":"2025-07-12T11:26:19.893009Z","caller":"traceutil/trace.go:171","msg":"trace[1654758925] transaction","detail":"{read_only:false; response_revision:366; number_of_response:1; }","duration":"116.027492ms","start":"2025-07-12T11:26:19.776949Z","end":"2025-07-12T11:26:19.892976Z","steps":["trace[1654758925] 'process raft request'  (duration: 108.749953ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:26:19.918434Z","caller":"traceutil/trace.go:171","msg":"trace[1617649244] transaction","detail":"{read_only:false; response_revision:365; number_of_response:1; }","duration":"182.04454ms","start":"2025-07-12T11:26:19.736330Z","end":"2025-07-12T11:26:19.918375Z","steps":["trace[1617649244] 'process raft request'  (duration: 141.354444ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:26:19.929585Z","caller":"traceutil/trace.go:171","msg":"trace[1757389533] transaction","detail":"{read_only:false; response_revision:364; number_of_response:1; }","duration":"168.523334ms","start":"2025-07-12T11:26:19.734854Z","end":"2025-07-12T11:26:19.903378Z","steps":["trace[1757389533] 'process raft request'  (duration: 72.399251ms)","trace[1757389533] 'compare'  (duration: 32.868426ms)"],"step_count":2}
{"level":"info","ts":"2025-07-12T11:26:19.950188Z","caller":"traceutil/trace.go:171","msg":"trace[373330992] linearizableReadLoop","detail":"{readStateIndex:373; appliedIndex:371; }","duration":"213.987163ms","start":"2025-07-12T11:26:19.736142Z","end":"2025-07-12T11:26:19.950130Z","steps":["trace[373330992] 'read index received'  (duration: 63.123089ms)","trace[373330992] 'applied index is now lower than readState.Index'  (duration: 150.860198ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-12T11:26:19.967026Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"230.844123ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/view\" limit:1 ","response":"range_response_count:1 size:2108"}
{"level":"info","ts":"2025-07-12T11:26:19.967179Z","caller":"traceutil/trace.go:171","msg":"trace[1857698367] range","detail":"{range_begin:/registry/clusterroles/view; range_end:; response_count:1; response_revision:368; }","duration":"231.083447ms","start":"2025-07-12T11:26:19.736069Z","end":"2025-07-12T11:26:19.967153Z","steps":["trace[1857698367] 'agreement among raft nodes before linearized reading'  (duration: 230.828293ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-12T11:26:20.188665Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.40625ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038546326284919 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/serviceaccounts/kube-system/default\" mod_revision:0 > success:<request_put:<key:\"/registry/serviceaccounts/kube-system/default\" value_size:112 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-07-12T11:26:20.195441Z","caller":"traceutil/trace.go:171","msg":"trace[507822977] transaction","detail":"{read_only:false; response_revision:370; number_of_response:1; }","duration":"202.654829ms","start":"2025-07-12T11:26:19.992741Z","end":"2025-07-12T11:26:20.195396Z","steps":["trace[507822977] 'process raft request'  (duration: 86.250782ms)","trace[507822977] 'compare'  (duration: 108.516177ms)"],"step_count":2}
{"level":"info","ts":"2025-07-12T11:26:20.227034Z","caller":"traceutil/trace.go:171","msg":"trace[669484570] transaction","detail":"{read_only:false; response_revision:371; number_of_response:1; }","duration":"147.183581ms","start":"2025-07-12T11:26:20.078852Z","end":"2025-07-12T11:26:20.226035Z","steps":["trace[669484570] 'process raft request'  (duration: 128.790512ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:26:30.039548Z","caller":"traceutil/trace.go:171","msg":"trace[1665592211] transaction","detail":"{read_only:false; response_revision:436; number_of_response:1; }","duration":"123.904599ms","start":"2025-07-12T11:26:29.913233Z","end":"2025-07-12T11:26:30.037138Z","steps":["trace[1665592211] 'process raft request'  (duration: 121.501855ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-12T11:27:11.286491Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"225.647083ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-07-12T11:27:11.287000Z","caller":"traceutil/trace.go:171","msg":"trace[643970711] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:487; }","duration":"226.364864ms","start":"2025-07-12T11:27:11.060573Z","end":"2025-07-12T11:27:11.286938Z","steps":["trace[643970711] 'count revisions from in-memory index tree'  (duration: 225.500901ms)"],"step_count":1}
{"level":"info","ts":"2025-07-12T11:30:35.009011Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-07-12T11:30:35.009410Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-07-12T11:30:42.022701Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2025-07-12T11:30:42.027728Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-12T11:30:42.028405Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-12T11:30:42.028685Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-12T11:30:42.039816Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-07-12T11:30:42.042334Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-12T11:30:42.043205Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-12T11:30:42.043289Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 11:53:10 up 46 min,  0 users,  load average: 5.26, 3.81, 2.59
Linux minikube 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [356ddc14baf5] <==
I0712 11:30:58.514664       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0712 11:30:58.517224       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0712 11:30:58.521997       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0712 11:30:58.530168       1 controller.go:78] Starting OpenAPI AggregationController
I0712 11:30:58.530349       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0712 11:30:58.530385       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0712 11:30:58.530472       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0712 11:30:58.548906       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0712 11:30:58.549037       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0712 11:30:58.549123       1 repairip.go:200] Starting ipallocator-repair-controller
I0712 11:30:58.549156       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0712 11:30:58.549657       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0712 11:30:58.550093       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0712 11:30:58.556316       1 controller.go:142] Starting OpenAPI controller
I0712 11:30:58.556502       1 controller.go:90] Starting OpenAPI V3 controller
I0712 11:30:58.562215       1 naming_controller.go:299] Starting NamingConditionController
I0712 11:30:58.562264       1 establishing_controller.go:81] Starting EstablishingController
I0712 11:30:58.562308       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0712 11:30:58.562324       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0712 11:30:58.562344       1 crd_finalizer.go:269] Starting CRDFinalizer
I0712 11:30:58.864152       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0712 11:30:58.864363       1 policy_source.go:240] refreshing policies
I0712 11:30:58.887887       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0712 11:30:58.907689       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0712 11:30:58.909019       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0712 11:30:58.929014       1 cache.go:39] Caches are synced for LocalAvailability controller
I0712 11:30:58.929980       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0712 11:30:58.936394       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:30:58.936666       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0712 11:30:58.945740       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0712 11:30:58.950206       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0712 11:30:58.950458       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0712 11:30:58.951803       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0712 11:30:58.951898       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0712 11:30:58.952365       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0712 11:30:58.952396       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0712 11:30:58.960269       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0712 11:30:59.006091       1 aggregator.go:171] initial CRD sync complete...
I0712 11:30:59.006128       1 autoregister_controller.go:144] Starting autoregister controller
I0712 11:30:59.006142       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0712 11:30:59.006154       1 cache.go:39] Caches are synced for autoregister controller
E0712 11:30:59.031230       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0712 11:30:59.555569       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0712 11:31:00.829912       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0712 11:31:02.278453       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:31:02.352711       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0712 11:31:02.355635       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0712 11:31:02.447899       1 controller.go:667] quota admission added evaluator for: endpoints
I0712 11:31:02.604989       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0712 11:36:13.167966       1 controller.go:667] quota admission added evaluator for: namespaces
I0712 11:40:58.876251       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:43:07.304636       1 alloc.go:328] "allocated clusterIPs" service="vote/redis-service" clusterIPs={"IPv4":"10.107.139.13"}
I0712 11:43:07.462214       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:45:51.191754       1 alloc.go:328] "allocated clusterIPs" service="vote/db-service" clusterIPs={"IPv4":"10.109.175.229"}
I0712 11:45:51.320351       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:48:16.362467       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:48:16.416244       1 alloc.go:328] "allocated clusterIPs" service="vote/vote-service" clusterIPs={"IPv4":"10.100.119.250"}
I0712 11:50:58.903577       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0712 11:52:16.150271       1 alloc.go:328] "allocated clusterIPs" service="vote/result-service" clusterIPs={"IPv4":"10.96.112.25"}
I0712 11:52:16.380198       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-apiserver [5f1fb9ac9538] <==
W0712 11:30:40.557706       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:40.613415       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:40.670507       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:40.758986       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:40.767108       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:40.796681       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.103237       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.207311       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.352743       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.498001       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.506893       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.532296       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.536678       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.571706       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.690120       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.700809       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.733038       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.758254       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.912772       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.924511       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.949287       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.953669       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:43.961545       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.041535       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.104285       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.108900       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.112998       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.201411       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.208650       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.208968       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.231906       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.239988       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.332499       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.368728       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.412354       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.440555       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.443528       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.472588       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.483511       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.494967       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.495917       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.517760       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.605242       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.617453       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.637688       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.646583       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.677344       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.697053       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.699768       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.717468       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.719032       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.760963       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.776105       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.836748       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.851721       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.870077       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.909917       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.962584       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:44.974754       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0712 11:30:45.091010       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [162557ea10aa] <==
I0712 11:26:16.932410       1 disruption.go:455] "Sending events to api server." logger="disruption-controller"
I0712 11:26:16.934609       1 disruption.go:466] "Starting disruption controller" logger="disruption-controller"
I0712 11:26:16.934642       1 shared_informer.go:350] "Waiting for caches to sync" controller="disruption"
I0712 11:26:17.031622       1 controllermanager.go:778] "Started controller" controller="persistentvolume-attach-detach-controller"
I0712 11:26:17.032621       1 attach_detach_controller.go:338] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
I0712 11:26:17.032859       1 shared_informer.go:350] "Waiting for caches to sync" controller="attach detach"
I0712 11:26:17.262443       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0712 11:26:17.386132       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0712 11:26:17.760099       1 shared_informer.go:357] "Caches are synced" controller="node"
I0712 11:26:17.761739       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0712 11:26:17.774703       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0712 11:26:17.775031       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0712 11:26:17.775371       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0712 11:26:17.784051       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0712 11:26:17.784201       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0712 11:26:17.784275       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0712 11:26:17.784361       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0712 11:26:17.784451       1 shared_informer.go:357] "Caches are synced" controller="job"
I0712 11:26:17.789726       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0712 11:26:17.791595       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0712 11:26:17.806293       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0712 11:26:17.806972       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0712 11:26:17.840826       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0712 11:26:17.849082       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0712 11:26:17.851362       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0712 11:26:17.851505       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0712 11:26:17.851608       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0712 11:26:17.851824       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0712 11:26:17.852072       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0712 11:26:17.883687       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0712 11:26:17.905009       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0712 11:26:17.915364       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0712 11:26:17.925709       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0712 11:26:17.950063       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0712 11:26:17.951534       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0712 11:26:17.951720       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0712 11:26:17.951960       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0712 11:26:17.952137       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0712 11:26:17.962691       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0712 11:26:17.965642       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0712 11:26:17.983136       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0712 11:26:17.986755       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0712 11:26:17.988409       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0712 11:26:17.995588       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0712 11:26:17.997932       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0712 11:26:18.016845       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0712 11:26:18.019064       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0712 11:26:18.025013       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0712 11:26:18.036364       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0712 11:26:18.036816       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0712 11:26:18.048263       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0712 11:26:18.049668       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0712 11:26:18.038131       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0712 11:26:18.037616       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0712 11:26:18.038474       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0712 11:26:18.182310       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0712 11:26:18.582536       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0712 11:26:18.589562       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0712 11:26:18.589611       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0712 11:26:18.589626       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-controller-manager [6c9e9cec2620] <==
I0712 11:31:01.891675       1 controllermanager.go:778] "Started controller" controller="deployment-controller"
I0712 11:31:01.892482       1 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
I0712 11:31:01.893445       1 shared_informer.go:350] "Waiting for caches to sync" controller="deployment"
I0712 11:31:01.937961       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0712 11:31:01.938030       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0712 11:31:01.987573       1 controllermanager.go:778] "Started controller" controller="bootstrap-signer-controller"
I0712 11:31:01.990070       1 shared_informer.go:350] "Waiting for caches to sync" controller="bootstrap_signer"
I0712 11:31:02.033800       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0712 11:31:02.064365       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0712 11:31:02.081711       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0712 11:31:02.087600       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0712 11:31:02.087892       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0712 11:31:02.088944       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0712 11:31:02.089321       1 shared_informer.go:357] "Caches are synced" controller="node"
I0712 11:31:02.089545       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0712 11:31:02.090017       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0712 11:31:02.090281       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0712 11:31:02.090530       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0712 11:31:02.095511       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0712 11:31:02.095740       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0712 11:31:02.095795       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0712 11:31:02.096039       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0712 11:31:02.098434       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0712 11:31:02.099091       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0712 11:31:02.099107       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0712 11:31:02.124137       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0712 11:31:02.142186       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0712 11:31:02.142695       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0712 11:31:02.144350       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0712 11:31:02.189591       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0712 11:31:02.206859       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0712 11:31:02.238352       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0712 11:31:02.267217       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0712 11:31:02.281284       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0712 11:31:02.287020       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0712 11:31:02.287516       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0712 11:31:02.287947       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0712 11:31:02.288154       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0712 11:31:02.290766       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0712 11:31:02.293465       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0712 11:31:02.294813       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0712 11:31:02.303550       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0712 11:31:02.306242       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0712 11:31:02.309158       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0712 11:31:02.315417       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0712 11:31:02.329253       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0712 11:31:02.329303       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0712 11:31:02.338918       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0712 11:31:02.338971       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0712 11:31:02.341244       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0712 11:31:02.342229       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0712 11:31:02.351887       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0712 11:31:02.359024       1 shared_informer.go:357] "Caches are synced" controller="job"
I0712 11:31:02.365315       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0712 11:31:02.397236       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0712 11:31:02.446545       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0712 11:31:02.822571       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0712 11:31:02.822647       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0712 11:31:02.822656       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0712 11:31:02.826692       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-proxy [c13ed318eb57] <==
I0712 11:26:28.779832       1 server_linux.go:63] "Using iptables proxy"
I0712 11:26:32.775443       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0712 11:26:32.777577       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0712 11:26:33.747372       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0712 11:26:33.820178       1 server_linux.go:145] "Using iptables Proxier"
I0712 11:26:34.119547       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0712 11:26:34.303774       1 server.go:516] "Version info" version="v1.33.1"
I0712 11:26:34.309173       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0712 11:26:34.391975       1 config.go:199] "Starting service config controller"
I0712 11:26:34.401698       1 config.go:105] "Starting endpoint slice config controller"
I0712 11:26:34.414010       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0712 11:26:34.415481       1 config.go:440] "Starting serviceCIDR config controller"
I0712 11:26:34.415503       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0712 11:26:34.419919       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0712 11:26:34.442782       1 config.go:329] "Starting node config controller"
I0712 11:26:34.443468       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0712 11:26:34.517246       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0712 11:26:34.523404       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0712 11:26:34.582076       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0712 11:26:34.621003       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [edbac3b7efc7] <==
I0712 11:30:56.257674       1 server_linux.go:63] "Using iptables proxy"
I0712 11:30:59.071235       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0712 11:30:59.071353       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0712 11:30:59.730583       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0712 11:30:59.734874       1 server_linux.go:145] "Using iptables Proxier"
I0712 11:30:59.782016       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0712 11:30:59.782952       1 server.go:516] "Version info" version="v1.33.1"
I0712 11:30:59.782978       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0712 11:30:59.850036       1 config.go:440] "Starting serviceCIDR config controller"
I0712 11:30:59.850074       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0712 11:30:59.850386       1 config.go:199] "Starting service config controller"
I0712 11:30:59.850401       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0712 11:30:59.850420       1 config.go:105] "Starting endpoint slice config controller"
I0712 11:30:59.850425       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0712 11:30:59.850744       1 config.go:329] "Starting node config controller"
I0712 11:30:59.850753       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0712 11:30:59.955572       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0712 11:30:59.957549       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0712 11:30:59.958129       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0712 11:30:59.974062       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [27fb3c386d66] <==
I0712 11:30:56.541295       1 serving.go:386] Generated self-signed cert in-memory
I0712 11:30:59.467301       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0712 11:30:59.467347       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0712 11:30:59.478630       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0712 11:30:59.478671       1 shared_informer.go:350] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0712 11:30:59.479994       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0712 11:30:59.480020       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0712 11:30:59.480079       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0712 11:30:59.480087       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0712 11:30:59.484220       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0712 11:30:59.498408       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0712 11:30:59.581027       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0712 11:30:59.581266       1 shared_informer.go:357] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0712 11:30:59.581307       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [3ec17cdcb3b5] <==
I0712 11:26:00.281328       1 serving.go:386] Generated self-signed cert in-memory
W0712 11:26:07.056152       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0712 11:26:07.056364       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0712 11:26:07.056412       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0712 11:26:07.056932       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0712 11:26:07.338540       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0712 11:26:07.338634       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0712 11:26:07.400769       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0712 11:26:07.403914       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0712 11:26:07.443518       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0712 11:26:07.446553       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0712 11:26:07.447929       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0712 11:26:07.488884       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0712 11:26:07.493466       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0712 11:26:07.493612       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0712 11:26:07.531278       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0712 11:26:07.550687       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0712 11:26:07.555636       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0712 11:26:07.560674       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0712 11:26:07.556891       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0712 11:26:07.557325       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0712 11:26:07.557503       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0712 11:26:07.557646       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0712 11:26:07.561018       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0712 11:26:07.561321       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0712 11:26:07.556288       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0712 11:26:07.561407       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0712 11:26:08.382395       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0712 11:26:08.415952       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0712 11:26:08.422188       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0712 11:26:08.423887       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0712 11:26:08.513338       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0712 11:26:08.643946       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0712 11:26:08.644710       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0712 11:26:08.798725       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0712 11:26:08.810679       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0712 11:26:08.833648       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0712 11:26:08.885256       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0712 11:26:08.888192       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0712 11:26:08.935234       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0712 11:26:08.975120       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0712 11:26:09.074299       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0712 11:26:09.155079       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0712 11:26:10.258437       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0712 11:26:10.435500       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0712 11:26:14.708561       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0712 11:30:35.101753       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.904814    2462 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.906005    2462 status_manager.go:895] "Failed to get status for pod" podUID="c7bc9044-4095-4d05-b70f-625e2fe06305" pod="kube-system/kube-proxy-wcbsz" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-wcbsz\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.908344    2462 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.911056    2462 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.915757    2462 status_manager.go:895] "Failed to get status for pod" podUID="3906195b-2291-4c68-8e45-2f2a6ea05c3c" pod="kube-system/coredns-674b8bbfcf-t42pm" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-t42pm\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.930588    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6244ea8bc524b9047e05bedcb6df4fb3955df8a4edba0d3b7e772535002d6e2e"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.932877    2462 scope.go:117] "RemoveContainer" containerID="f4fa9acb24bb26299bb38c1e5d5f7c9e261d9fd2f5581df6525dd555ae81ec36"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.934279    2462 status_manager.go:895] "Failed to get status for pod" podUID="2366996a-db12-462e-9f86-180d8ee70d49" pod="kube-system/storage-provisioner" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.938136    2462 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.938326    2462 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.938448    2462 status_manager.go:895] "Failed to get status for pod" podUID="3906195b-2291-4c68-8e45-2f2a6ea05c3c" pod="kube-system/coredns-674b8bbfcf-t42pm" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-t42pm\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.938612    2462 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.938726    2462 status_manager.go:895] "Failed to get status for pod" podUID="c7bc9044-4095-4d05-b70f-625e2fe06305" pod="kube-system/kube-proxy-wcbsz" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-wcbsz\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.967493    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f4ecd20aa4ebe575f822905e44d32a161c6397996eefb5670f6dd1e96ffafebd"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.970015    2462 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.970408    2462 status_manager.go:895] "Failed to get status for pod" podUID="c7bc9044-4095-4d05-b70f-625e2fe06305" pod="kube-system/kube-proxy-wcbsz" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-wcbsz\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.970556    2462 status_manager.go:895] "Failed to get status for pod" podUID="2366996a-db12-462e-9f86-180d8ee70d49" pod="kube-system/storage-provisioner" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.970664    2462 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.970769    2462 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:48 minikube kubelet[2462]: I0712 11:30:48.979559    2462 status_manager.go:895] "Failed to get status for pod" podUID="3906195b-2291-4c68-8e45-2f2a6ea05c3c" pod="kube-system/coredns-674b8bbfcf-t42pm" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-t42pm\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:48.999342    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2389728a479a3cad14e4567a347d066949d8c69da00a68c194f568e2e6a3f843"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.002759    2462 status_manager.go:895] "Failed to get status for pod" podUID="3906195b-2291-4c68-8e45-2f2a6ea05c3c" pod="kube-system/coredns-674b8bbfcf-t42pm" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-t42pm\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.003102    2462 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.003303    2462 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.003477    2462 status_manager.go:895] "Failed to get status for pod" podUID="c7bc9044-4095-4d05-b70f-625e2fe06305" pod="kube-system/kube-proxy-wcbsz" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-wcbsz\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.003692    2462 status_manager.go:895] "Failed to get status for pod" podUID="2366996a-db12-462e-9f86-180d8ee70d49" pod="kube-system/storage-provisioner" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.004055    2462 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:49 minikube kubelet[2462]: I0712 11:30:49.004266    2462 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Jul 12 11:30:50 minikube kubelet[2462]: I0712 11:30:50.441962    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d25c9271969fc08e53d7bf21e3a509e2d23855dc3b601de3b8fc8d0c2f38390b"
Jul 12 11:30:50 minikube kubelet[2462]: I0712 11:30:50.606114    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="119227a075c9035837921bdfb7fccb888279e1819d2558bf0f1ee968757ca57e"
Jul 12 11:30:51 minikube kubelet[2462]: I0712 11:30:51.740415    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5a91c1d47db89cd579c44279b02aa6ae87f2ea9a326f203a73afabd9da0ec52e"
Jul 12 11:30:51 minikube kubelet[2462]: I0712 11:30:51.972415    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d3f4d637fa8c9e2ae971b7a9db3e5b8794b0ac6f5ba1fee760b2c8761bda77ef"
Jul 12 11:30:52 minikube kubelet[2462]: I0712 11:30:52.323022    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7931ce28e4f5b6994b539dd4943921a60c1a569de0059ca9604afc7043baffa2"
Jul 12 11:30:58 minikube kubelet[2462]: E0712 11:30:58.708268    2462 reflector.go:200] "Failed to watch" err="configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError" reflector="object-\"kube-system\"/\"kube-root-ca.crt\"" type="*v1.ConfigMap"
Jul 12 11:30:58 minikube kubelet[2462]: E0712 11:30:58.708364    2462 reflector.go:200] "Failed to watch" err="configmaps \"kube-proxy\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError" reflector="object-\"kube-system\"/\"kube-proxy\"" type="*v1.ConfigMap"
Jul 12 11:30:58 minikube kubelet[2462]: E0712 11:30:58.708400    2462 reflector.go:200] "Failed to watch" err="configmaps \"coredns\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError" reflector="object-\"kube-system\"/\"coredns\"" type="*v1.ConfigMap"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.710714    2462 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="pods \"kube-scheduler-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.729370    2462 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="pods \"kube-controller-manager-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.790729    2462 status_manager.go:895] "Failed to get status for pod" podUID="c7bc9044-4095-4d05-b70f-625e2fe06305" pod="kube-system/kube-proxy-wcbsz" err="pods \"kube-proxy-wcbsz\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.825007    2462 status_manager.go:895] "Failed to get status for pod" podUID="2366996a-db12-462e-9f86-180d8ee70d49" pod="kube-system/storage-provisioner" err="pods \"storage-provisioner\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.837126    2462 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="pods \"etcd-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.840031    2462 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="pods \"kube-apiserver-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.842472    2462 status_manager.go:895] "Failed to get status for pod" podUID="3906195b-2291-4c68-8e45-2f2a6ea05c3c" pod="kube-system/coredns-674b8bbfcf-t42pm" err="pods \"coredns-674b8bbfcf-t42pm\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.855095    2462 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="pods \"kube-scheduler-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.859790    2462 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="pods \"kube-controller-manager-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:30:58 minikube kubelet[2462]: I0712 11:30:58.861970    2462 status_manager.go:895] "Failed to get status for pod" podUID="c7bc9044-4095-4d05-b70f-625e2fe06305" pod="kube-system/kube-proxy-wcbsz" err="pods \"kube-proxy-wcbsz\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Jul 12 11:42:18 minikube kubelet[2462]: I0712 11:42:18.786492    2462 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kzbjh\" (UniqueName: \"kubernetes.io/projected/aaeca633-7dfd-430b-8a9a-e4d678bbe4c0-kube-api-access-kzbjh\") pod \"redis-7676fddfbb-7pcdg\" (UID: \"aaeca633-7dfd-430b-8a9a-e4d678bbe4c0\") " pod="vote/redis-7676fddfbb-7pcdg"
Jul 12 11:45:34 minikube kubelet[2462]: I0712 11:45:34.959417    2462 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="vote/redis-7676fddfbb-7pcdg" podStartSLOduration=164.19264909 podStartE2EDuration="3m16.950176913s" podCreationTimestamp="2025-07-12 11:42:18 +0000 UTC" firstStartedPulling="2025-07-12 11:42:20.228682205 +0000 UTC m=+966.553517312" lastFinishedPulling="2025-07-12 11:42:52.986210007 +0000 UTC m=+999.311045135" observedRunningTime="2025-07-12 11:42:54.589640113 +0000 UTC m=+1000.914475232" watchObservedRunningTime="2025-07-12 11:45:34.950176913 +0000 UTC m=+1161.275012082"
Jul 12 11:45:35 minikube kubelet[2462]: I0712 11:45:35.167675    2462 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wlqdw\" (UniqueName: \"kubernetes.io/projected/be60b413-812d-4723-a708-abba39bac5a8-kube-api-access-wlqdw\") pod \"postgres-9b97bcfdc-7qx5d\" (UID: \"be60b413-812d-4723-a708-abba39bac5a8\") " pod="vote/postgres-9b97bcfdc-7qx5d"
Jul 12 11:45:36 minikube kubelet[2462]: I0712 11:45:36.012484    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3c04d5c212a3aade4238cc0bfc97a88d7246e27fe9ad3dd19d2f0a400dee4884"
Jul 12 11:47:56 minikube kubelet[2462]: I0712 11:47:56.870172    2462 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="vote/postgres-9b97bcfdc-7qx5d" podStartSLOduration=72.734401775 podStartE2EDuration="2m22.870097997s" podCreationTimestamp="2025-07-12 11:45:34 +0000 UTC" firstStartedPulling="2025-07-12 11:45:36.49342823 +0000 UTC m=+1162.818263311" lastFinishedPulling="2025-07-12 11:46:46.62912441 +0000 UTC m=+1232.953959533" observedRunningTime="2025-07-12 11:46:51.975541894 +0000 UTC m=+1238.300377016" watchObservedRunningTime="2025-07-12 11:47:56.870097997 +0000 UTC m=+1303.194933126"
Jul 12 11:47:56 minikube kubelet[2462]: I0712 11:47:56.950121    2462 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4lshm\" (UniqueName: \"kubernetes.io/projected/3dc9e899-083c-4eac-aee1-e06bc2f04a0f-kube-api-access-4lshm\") pod \"vote-app-79b7b95c55-m8dvw\" (UID: \"3dc9e899-083c-4eac-aee1-e06bc2f04a0f\") " pod="vote/vote-app-79b7b95c55-m8dvw"
Jul 12 11:47:57 minikube kubelet[2462]: I0712 11:47:57.056037    2462 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5c7kx\" (UniqueName: \"kubernetes.io/projected/02d7ce38-8f98-47ca-99d0-6be102ab41a2-kube-api-access-5c7kx\") pod \"vote-app-79b7b95c55-6sl6k\" (UID: \"02d7ce38-8f98-47ca-99d0-6be102ab41a2\") " pod="vote/vote-app-79b7b95c55-6sl6k"
Jul 12 11:47:58 minikube kubelet[2462]: I0712 11:47:58.614140    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4d0bcb7d2b4cd4746220fa37534be54393979cc5d250110adefd63345a2b5f29"
Jul 12 11:48:31 minikube kubelet[2462]: I0712 11:48:31.505975    2462 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="vote/vote-app-79b7b95c55-m8dvw" podStartSLOduration=5.157139301 podStartE2EDuration="35.505852137s" podCreationTimestamp="2025-07-12 11:47:56 +0000 UTC" firstStartedPulling="2025-07-12 11:47:59.113974797 +0000 UTC m=+1305.438809880" lastFinishedPulling="2025-07-12 11:48:29.462687594 +0000 UTC m=+1335.787522716" observedRunningTime="2025-07-12 11:48:31.50540275 +0000 UTC m=+1337.830237879" watchObservedRunningTime="2025-07-12 11:48:31.505852137 +0000 UTC m=+1337.830687271"
Jul 12 11:52:04 minikube kubelet[2462]: I0712 11:52:04.306590    2462 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="vote/vote-app-79b7b95c55-6sl6k" podStartSLOduration=214.630080565 podStartE2EDuration="4m8.29267576s" podCreationTimestamp="2025-07-12 11:47:56 +0000 UTC" firstStartedPulling="2025-07-12 11:47:59.230128036 +0000 UTC m=+1305.554963139" lastFinishedPulling="2025-07-12 11:48:32.892723224 +0000 UTC m=+1339.217558334" observedRunningTime="2025-07-12 11:48:33.785312646 +0000 UTC m=+1340.110147787" watchObservedRunningTime="2025-07-12 11:52:04.29267576 +0000 UTC m=+1550.617510880"
Jul 12 11:52:04 minikube kubelet[2462]: I0712 11:52:04.522020    2462 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-knm92\" (UniqueName: \"kubernetes.io/projected/b898c8af-2a68-4c71-abc6-f318f94f44a5-kube-api-access-knm92\") pod \"result-app-7cc59d596-wmtm4\" (UID: \"b898c8af-2a68-4c71-abc6-f318f94f44a5\") " pod="vote/result-app-7cc59d596-wmtm4"
Jul 12 11:52:04 minikube kubelet[2462]: I0712 11:52:04.522682    2462 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pf8ft\" (UniqueName: \"kubernetes.io/projected/3646f226-8ddd-445e-8cea-ab7c8f3d58fc-kube-api-access-pf8ft\") pod \"result-app-7cc59d596-7p2fc\" (UID: \"3646f226-8ddd-445e-8cea-ab7c8f3d58fc\") " pod="vote/result-app-7cc59d596-7p2fc"
Jul 12 11:52:06 minikube kubelet[2462]: I0712 11:52:06.678833    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8fde1a9889dc113d70b27de3e293a7f90d9b31adbca18d5fd96dcc679877457e"
Jul 12 11:52:06 minikube kubelet[2462]: I0712 11:52:06.869588    2462 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="858d31813f03a10024f75e23934edf080af6a345e01b6d7805f1a1ecefbd85cc"


==> storage-provisioner [260235fe70dd] <==
W0712 11:52:10.901321       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:10.933222       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:12.946073       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:13.000390       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:15.022948       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:15.077291       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:17.099078       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:17.131382       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:19.145164       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:19.197286       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:21.241238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:21.290773       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:23.332694       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:23.379788       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:25.395647       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:25.445382       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:27.466831       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:27.505240       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:29.516999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:29.546522       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:31.572983       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:31.650706       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:33.662327       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:33.675045       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:35.696806       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:35.736103       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:37.756786       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:37.780402       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:39.790767       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:39.810943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:41.825607       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:41.848188       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:43.864127       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:43.889394       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:45.914236       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:45.963079       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:47.989909       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:48.029963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:50.041280       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:50.077520       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:52.095771       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:52.125558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:54.148284       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:54.199253       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:56.211128       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:56.243766       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:58.255681       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:52:58.300364       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:00.319682       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:00.347130       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:02.361938       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:02.399817       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:04.452757       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:04.546708       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:06.569089       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:06.621118       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:08.635096       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:08.649994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:10.657767       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:53:10.705572       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [ed345f136c27] <==
W0712 11:29:34.435604       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:34.454233       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:36.464839       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:36.480208       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:38.490537       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:38.505561       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:40.526669       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:40.555333       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:42.564114       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:42.582483       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:44.593369       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:44.609125       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:46.619833       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:46.635108       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:48.644985       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:48.659978       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:50.686411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:50.724260       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:52.736030       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:52.755752       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:54.780205       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:54.794420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:56.802639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:56.818091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:58.828598       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:29:58.841852       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:00.854674       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:00.875469       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:02.886351       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:02.900440       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:04.912571       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:04.932150       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:06.941578       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:06.962433       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:08.972031       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:08.994523       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:11.011930       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:11.027530       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:13.037055       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:13.049438       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:15.062026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:15.083590       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:17.097420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:17.123173       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:19.134528       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:19.155469       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:21.182612       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:21.212845       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:23.220694       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:23.240936       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:25.249778       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:25.269008       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:27.300472       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:27.339217       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:29.354073       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:29.375005       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:31.395633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:31.469145       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:33.477593       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0712 11:30:33.496114       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

